# Real-Time Gesture Mimicry
 Controlling a 3D-Printed Robotic Hand Using Webcam-Based Hand Tracking

#### disclaimer: This code is based on a code template generated by Claude AI that i have made a lot of improvments and changes to, I am a complete beginner in python (and coding in general) and still learning.

This project is a part of my learning journey (and garduation project!), so the end goal is to be able to rewrite the whole code from scratch all by myself, and honestly I wasn't planning on making this repo public until I rewrite the whole project from scratch but hmm... I wanna experiment and see if making it public now will help me in any way and I guess it could help others too so let's see how this will go!

This project uses the following for hand recognition and tracking:
- mediapipe
- openCV

#### Hint: to view the latest code changes and updates check the testing branch

The code is split across 3 different files:
- simple_hand_tracker.py: contains all the necessary code for starting the camera, detecting the hand and calculating the openness
- hand_control_system.py: translates the openness value to servo motor angle then send it to the Arduino
- Arduino_code.ino: sets up the Arduino to receive the servo motor angles and control the connected servo motors accordingly

---

# road map
- [x] seperate the thumb from the rest
- [x] translate all openness values to angles
- [x] add depth preception to greatly increase the accuracy (if possible)
- [x] auto detect host os to auto adjust Arduino port path (kinda done, needs testing)
- [x] calculate thumb openness based on its distence from the base of the ring finger?
- [ ] decouple the effects of hand orientation from the openness metric
     - [ ] understand the math
- [ ] make it use the intel iGPU for inferece?
- [ ] write a proper documentation for the project
- [ ] extend the code to control each finger seperetly (WIP)
- [ ] refactor, optimize, etc...
- [ ] rewrite whole project from scratch all by myself
